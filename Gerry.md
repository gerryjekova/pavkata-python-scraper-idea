# This is my notes and anything important I need to disclose and link connected to the process in this exercise will be most likely noted here.

### ðŸ“ Resources and ðŸ”—links: 
- [LLM Scraper Repo](https://github.com/mishushakov/llm-scraper)
  - [Shortcut to Code-generation feature information -For LLM Scraper-](https://github.com/mishushakov/llm-scraper?tab=readme-ov-file#code-generation)
- [Crawl4AI Library](https://github.com/unclecode/crawl4ai)
- [Python Code and Project on Replit](https://replit.com/@gerryjekova/WryWhimsicalGigahertz)
- [Copilot in Github Chat](https://github.com/copilot/share/48490026-00a0-8884-a941-5e49e09228c8)


### Exercise breakdown & solutions
> 1. Project Overview
>> The goal of this project is to develop a Python-based API that accepts a URL as input and returns scraped content from a news/media website. The scraper must dynamically generate or retrieve extraction rules (in JSON format) for each domain and then extract the following media fields (if available):

- **Title**
- **Content**
- **Author**
- **Publish Date**
- **Language**
- **Categories** (multiple allowed)
- **Media Files**:
  - Images
  - Videos
  - Embeds

--- 
1. My notes - A json format file that must **dynamically** retrieve extraction rules in a JSON format ? 
> I am a little confused on what that means, but I am assuming it means something like 

``` extraction.json
domain.com: {
    title: 
    content:
    author: 
    publish date: 
    language: 
    categories: 
    media files: { 
        images: 
        videos: 
        embeds: 
    }
}
```

# 2 chat 
```
project/
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ config.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ queue_manager.py
â”‚   â”œâ”€â”€ schema_generator.py
â”‚   â”œâ”€â”€ scraper.py
â”‚   â””â”€â”€ storage.py
â””â”€â”€ rules/
    â””â”€â”€ domains/
        â””â”€â”€ .gitkeep
```

# 3 implementation 
This implementation includes:

POST /scrape

Accepts URL and optional parameters (headers, timeout)
Returns a task ID and initial status
Validates input and handles errors
Returns 202 Accepted status code
GET /scrape/{task_id}

Returns task status and results
Includes error handling
Returns appropriate HTTP status codes
Provides detailed task information
Additional features:

Proper error handling and logging
Input validation
Structured response models
Clear status tracking
Optional parameter support
Would you like me to:

Add request rate limiting?
Implement authentication?
Add more optional parameters?
Show example API usage?
Let me know what aspect you'd like to focus on next!

To use this system:

Start the Redis server:
bash
redis-server
Start Celery workers:
bash
celery -A main.celery worker --loglevel=info
Start the Flask application:
bash
python main.py


The scraper can be used in two ways:

With Crawl4AI (recommended):
Python
scraper = Scraper(crawl4ai_client=crawl4ai_instance)
result = await scraper.scrape(url, schema)
Direct scraping:
Python
scraper = Scraper()
result = await scraper.scrape(
    url,
    schema,
    headers={'User-Agent': 'Custom UA'},
    timeout=30
)

Project structure after Technical Req:
```
scraper_api/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logging.py
â”‚   â”‚   â””â”€â”€ settings.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ router.py
â”‚   â”‚   â””â”€â”€ endpoints/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ scraping.py
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ domain.py
â”‚   â”‚   â”œâ”€â”€ scraping.py
â”‚   â”‚   â””â”€â”€ task.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ crawler.py
â”‚   â”‚   â”œâ”€â”€ queue.py
â”‚   â”‚   â””â”€â”€ storage.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_api/
â”‚   â”œâ”€â”€ test_crawler/
â”‚   â””â”€â”€ test_queue/
â”œâ”€â”€ alembic/
â”‚   â””â”€â”€ versions/
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```